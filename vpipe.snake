import csv
import collections
import configparser
import os
import sys
import typing

# 1. parse config file
#
# the config object sets defaults for a number of parameters and
# validates parameters given by the user to be sensible

VPIPE_DEBUG = True if os.environ.get('VPIPE_DEBUG') is not None else False


class VpipeConfig(object):
    'Class used to encapsulate the configuration properties used by V-pipe'

    __RECORD__ = typing.NamedTuple(
        "__RECORD__", [('value', typing.Any), ('type', type)])
    __MEMBER_DEFAULT__ = collections.OrderedDict([
        ('general', {
            'threads': __RECORD__(value=4, type=int),
            'aligner': __RECORD__(value='ngshmmalign', type=str),
        }),
        ('input', {
            'datadir': __RECORD__(value='samples', type=str),
            'samples_file': __RECORD__(value='samples.tsv', type=str),
            'fastq_suffix': __RECORD__(value='', type=str),
            'trim_percent_cutoff': __RECORD__(value=0.8, type=float),
            'reference': __RECORD__(value='references/HXB2.fasta', type=str),
        }),
        ('output', {
            'QA': __RECORD__(value=False, type=bool),
            'snv': __RECORD__(value=True, type=bool),
            'local': __RECORD__(value=True, type=bool),
            'global': __RECORD__(value=True, type=bool),
        }),
        ('applications', {
            'gunzip': __RECORD__(value="gunzip", type=str),
            'prinseq': __RECORD__(value="prinseq-lite.pl", type=str),
            'vicuna': __RECORD__(value="vicuna", type=str),
            'indelfixer': __RECORD__(value="IndelFixer.jar", type=str),
            'consensusfixer': __RECORD__(value="ConsensusFixer.jar", type=str),
            'picard': __RECORD__(value="picard", type=str),
            'bwa': __RECORD__(value="bwa", type=str),
            'samtools': __RECORD__(value="samtools", type=str),
            'mafft': __RECORD__(value="mafft", type=str),
            'ngshmmalign': __RECORD__(value="ngshmmalign", type=str),
            'convert_reference': __RECORD__(value="convert_reference", type=str),
            'extract_seq': __RECORD__(value="extract_seq", type=str),
            'coverage_stats': __RECORD__(value="coverage_stats", type=str),
            'remove_gaps_msa': __RECORD__(value="remove_gaps_msa", type=str),
            'minority_freq': __RECORD__(value="minority_freq", type=str),
            'extract_coverage_intervals': __RECORD__(value="extract_coverage_intervals", type=str),
            'shorah': __RECORD__(value="dec.py", type='str'),
            'haploclique': __RECORD__(value="haploclique", type='str'),
            'compute_mds': __RECORD__(value="compute_mds", type='str')
        }),

        ('gunzip', {
            'mem': __RECORD__(value=30000, type=int),
            'time': __RECORD__(value=60, type=int),
        }),
        ('extract', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=20, type=int),
        }),
        ('preprocessing', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='', type=str),

            'qual_threshold': __RECORD__(value=30, type=int),
            'min_len': __RECORD__(value=0.8, type=float),
        }),
        ('initial_vicuna', {
            'mem': __RECORD__(value=1000, type=int),
            'time': __RECORD__(value=600, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('initial_vicuna_msa', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('create_vicuna_initial', {
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('hmm_align', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=1435, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),

            'leave_msa_temp': __RECORD__(value=False, type=bool),
        }),
        ('sam2bam', {
            'mem': __RECORD__(value=5000, type=int),
            'time': __RECORD__(value=30, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('bwa_QA', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('coverage_QA', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('msa', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('convert_to_ref', {
            'mem': __RECORD__(value=8000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('ref_index', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/bwa_align.yaml', type=str),
        }),
        ('bwa_align', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('minor_variants', {
            'mem': __RECORD__(value=1000, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('coverage_intervals', {
            'mem': __RECORD__(value=1000, type=int),
            'time': __RECORD__(value=60, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
            
            'coverage': __RECORD__(value=50, type=int),
            'liberal': __RECORD__(value=True, type=bool),
        }),
        ('snv', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=2880, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),

            'shift': __RECORD__(value=3, type=int),
            'keep_files': __RECORD__(value=False, type=bool),
        }),
        ('aggregate', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
        }),
        ('haploclique', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=1435, type=int),
            'conda': __RECORD__(value='', type=str),

            'relax': __RECORD__(value=True, type=bool),
            'no_singletons': __RECORD__(value=True, type=bool),
            'no_prob0': __RECORD__(value=True, type=bool),
            'clique_size_limit': __RECORD__(value=3, type=int),
            'max_num_cliques': __RECORD__(value=10000, type=int),
        }),
        ('haploclique_visualization', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),

            'region_start': __RECORD__(value=0, type=int),
            'region_end': __RECORD__(value=9719, type=int),
            'msa': __RECORD__(value='', type=str),
        })
    ])

    def __init__(self):
        self.__members = {}

        vpipe_configfile = configparser.ConfigParser()
        vpipe_configfile.read('vpipe.config')

        for (section, properties) in VpipeConfig.__MEMBER_DEFAULT__.items():
            self.__members[section] = {}

            for (value, defaults) in properties.items():
                try:
                    if defaults.type == int:
                        cur_value = vpipe_configfile.getint(section, value)
                    elif defaults.type == float:
                        cur_value = vpipe_configfile.getfloat(section, value)
                    elif defaults.type == bool:
                        cur_value = vpipe_configfile.getboolean(section, value)
                    else:
                        cur_value = vpipe_configfile.get(section, value)
                    state = 'user'
                except (configparser.NoSectionError, configparser.NoOptionError):
                    if value == 'threads' and section != 'general':
                        cur_value = defaults.value if defaults.value else self.__members[
                            'general']['threads']
                    elif value == 'conda':
                        cur_value = "envs/{}.yaml".format(section) if len(
                            defaults.value) == 0 else defaults.value
                    else:
                        cur_value = defaults.value
                    state = 'DEFAULT'
                except ValueError:
                    print("ERROR: Property '{}' of section '{}' has to be of type '{}', whereas you gave '{}'!".format(
                        value, section, defaults.type.__name__, vpipe_configfile[section][value]))
                    raise

                if VPIPE_DEBUG:
                    print("Using {} value '{}' for property '{}' in section '{}'".format(
                        state, cur_value, value, section))

                self.__members[section][value] = cur_value

    def __getattr__(self, name):
        try:
            return self.__members[name]
        except:
            print("ERROR: Section '{}' is not a valid section!".format(name))
            raise


config = VpipeConfig()


# 2. glob patients/samples + store as TSV if file is not provided

# if file containing samples exists, proceed
# to build list of target files
if not os.path.isfile(config.input['samples_file']):
    # sample file does not exist, have to first glob
    # all patients' data and then construct sample
    # list that would pass QA checks

    patient_sample_pairs = glob_wildcards(
        "{}/{{patient_date}}/raw_data/{{file}}_R1{}.fastq.gz".format(config.input['datadir'], config.input['fastq_suffix']))

    with open('samples.tsv', 'w') as outfile:
        for i in patient_sample_pairs.patient_date:
            (patient, date) = [x.strip() for x in i.split("/") if x.strip()]
            outfile.write('{}\t{}\n'.format(patient, date))

    # TODO: have to preprocess patient files to filter likely failures
    # 1.) Determine 5%/95% length of FASTQ files
    # 2.) Determine whether FASTQ would survive


# 3. load patients from TSV and create list of samples
#
# This list is reused on subsequent runs

patient_list = []
patient_dict = {}
patient_record = typing.NamedTuple(
    "patient_record", [('patient_id', str), ('date', str)])

with open(config.input['samples_file'], newline='') as csvfile:
    spamreader = csv.reader(csvfile, delimiter='\t')

    for row in spamreader:
        assert len(row) >= 2, "ERROR: Line '{}' does not contain at least two entries!".format(
                              spamreader.line_num)
        patient_tuple = patient_record(patient_id=row[0], date=row[1])
        patient_list.append(patient_tuple)

        assert config.input['trim_percent_cutoff'] > 0 and config.input['trim_percent_cutoff'] < 1, "ERROR: 'trim_percent_cutoff' is expected to be a fraction (between 0 and 1), whereas 'trim_percent_cutoff'={}".format(
            config.input['trim_percent_cutoff'])
        assert patient_tuple not in patient_dict, "ERROR: sample '{}-{}' is not unique".format(
            row[0], row[1])

        if len(row) == 2:
            # All samples are assumed to have same read length and the default, 250 bp
            patient_dict[patient_tuple] = 250

        elif len(row) == 3:
            # Extract read length from input.samples_file. Samples may have
            # different read lengths. Reads will be filtered out if read length
            # after trimming is less than trim_cutoff * read_length.
            patient_dict[patient_tuple] = int(row[2])

# 4. generate list of target files
all_files = []
alignments = []
vicuna_refs = []
references = []
trimmed_files = []
results = []
datasets = []
IDs = []
for p in patient_list:

    alignments.append(
        "{sample_dir}/{patient}/{date}/alignments/REF_aln.bam".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    if config.output['QA']:
        alignments.append(
            "{sample_dir}/{patient}/{date}/QA_alignments/coverage_ambig.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
        alignments.append(
            "{sample_dir}/{patient}/{date}/QA_alignments/coverage_majority.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    vicuna_refs.append(
        "{sample_dir}/{patient}/{date}/references/vicuna_consensus.fasta".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    references.append(
        "{sample_dir}/{patient}/{date}/references/ref_".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    trimmed_files.append(
        "{sample_dir}/{patient}/{date}/preprocessed_data/R1.fastq.gz".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    trimmed_files.append(
        "{sample_dir}/{patient}/{date}/preprocessed_data/R2.fastq.gz".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    datasets.append("{sample_dir}/{patient}/{date}".format(
        sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    IDs.append(('{}-{}').format(p.patient_id, p.date))

    # SNV
    if config.output['snv']:
        results.append(
            "{sample_dir}/{patient}/{date}/variants/SNVs/snvs.csv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    # local haplotypes
    if config.output['local']:
        results.append(
            "{sample_dir}/{patient}/{date}/variants/local/snvs.csv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    # global haplotypes
    if config.output['global']:
        results.append(
            "{sample_dir}/{patient}/{date}/variants/global/quasispecies.bam".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    # merge lists contaiing expected output
    all_files = alignments + results

IDs = ','.join(IDs)

# 5. Locate reference and parse reference identifier
try:
    if os.path.isfile(config.input['reference']):
        reference_file = config.input['reference']
    elif os.path.isfile(os.path.join("references", config.input['reference'])):
        reference_file = os.path.join("references", config.input['reference'])
except:
    print("ERROR: Reference file {} does not exists".format(
        config.input['reference']))
    raise

with open(reference_file, 'r') as infile:
    reference_name = infile.readline().rstrip()
reference_name = reference_name.split('>')[1]
reference_name = reference_name.split(' ')[0]

# DUMMY RULES
rule all:
    input:
        all_files

rule alltrimmed:
    input:
        trimmed_files


# 1. extract
rule gunzip:
    input:
        "{file}.fastq.gz"
    output:
        temp("{file}.fastq")
    params:
        scratch = '10000',
        mem = config.gunzip['mem'],
        time = config.gunzip['time'],
        GUNZIP = config.applications['gunzip'],
    log:
        outfile = "/dev/null",
        errfile = "/dev/null",
    threads:
        1
    shell:
        """
        {params.GUNZIP} -c {input} > {output}
        """


def construct_input_fastq(wildcards):
    inferred_values = glob_wildcards(
        wildcards.dataset + "/raw_data/{file}R" + wildcards.pair + config.input['fastq_suffix'] + ".fastq.gz")
    if len(inferred_values.file) == 0:
        inferred_values = glob_wildcards(
            wildcards.dataset + "/raw_data/{file}R" + wildcards.pair + config.input['fastq_suffix'] + ".fastq")

    list_output = []

    for i in inferred_values.file:
        list_output.append(wildcards.dataset + "/raw_data/" +
                           i + "R" + wildcards.pair + config.input['fastq_suffix'] + ".fastq")
    if len(list_output) == 0:
        print("Missing input files for rule extract: {}/raw_data/ - Unexpected file name?".format(wildcards.dataset))
        sys.exit(1)

    return list_output


rule extract:
    input:
        construct_input_fastq
    output:
        temp("{dataset}/extracted_data/R{pair}.fastq")
    params:
        scratch = '2000',
        mem = config.extract['mem'],
        time = config.extract['time'],
    log:
        outfile = "{dataset}/extracted_data/extract_R{pair}.out.log",
        errfile = "{dataset}/extracted_data/extract_R{pair}.err.log"
    benchmark:
        "{dataset}/extracted_data/extract.benchmark"
    threads:
        1
    shell:
        """
        cat {input} | paste - - - - | sort -k1,1 -t " " | tr "\t" "\n" > {output} 2> >(tee {log.errfile} >&2)
        """

rule extractclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/extracted_data
        """


# 2. clipping
def len_cutoff(wildcards):
    parts = wildcards.dataset.split('/')
    patient_ID = parts[1]
    date = parts[2]
    patient_tuple = patient_record(patient_id=patient_ID, date=date)
    read_len = patient_dict[patient_tuple]
    len_cutoff = int(config.input['trim_percent_cutoff'] * read_len)
    return len_cutoff


rule preprocessing:
    input:
        R1 = "{dataset}/extracted_data/R1.fastq",
        R2 = "{dataset}/extracted_data/R2.fastq"
    output:
        R1gz = "{dataset}/preprocessed_data/R1.fastq.gz",
        R2gz = "{dataset}/preprocessed_data/R2.fastq.gz"
    params:
        scratch = '2000',
        mem = config.preprocessing['mem'],
        time = config.preprocessing['time'],
        LEN_CUTOFF = len_cutoff,
        PRINSEQ = config.applications['prinseq'],
    log:
        outfile = "{dataset}/preprocessed_data/prinseq.out.log",
        errfile = "{dataset}/preprocessed_data/prinseq.err.log"
    conda:
        config.preprocessing['conda']
    benchmark:
        "{dataset}/preprocessed_data/prinseq.benchmark"
    threads:
        1
    shell:
        """
        echo "The length cutoff is: {params.LEN_CUTOFF}" > {log.outfile}

        {params.PRINSEQ} -fastq {input.R1} -fastq2 {input.R2} -out_format 3 -out_good {wildcards.dataset}/preprocessed_data/R -out_bad null -ns_max_n 4 -min_qual_mean 30 -trim_qual_left 30 -trim_qual_right 30 -trim_qual_window 10 -min_len {params.LEN_CUTOFF}  -log {log.outfile} 2> >(tee {log.errfile} >&2) 

        mv {wildcards.dataset}/preprocessed_data/R{{_,}}1.fastq
        mv {wildcards.dataset}/preprocessed_data/R{{_,}}2.fastq
        rm -f {wildcards.dataset}/preprocessed_data/R_?_singletons.fastq

        gzip {wildcards.dataset}/preprocessed_data/R1.fastq
        gzip {wildcards.dataset}/preprocessed_data/R2.fastq
        """

rule trimmingclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/preprocessed_data
        """


# 3. initial consensus sequence
rule initial_vicuna:
    input:
        global_ref = reference_file,
        R1 = "{dataset}/preprocessed_data/R1.fastq",
        R2 = "{dataset}/preprocessed_data/R2.fastq"
    output:
        "{dataset}/references/vicuna_consensus.fasta"
    params:
        scratch = '1000',
        mem = config.initial_vicuna['mem'],
        time = config.initial_vicuna['time'],
        VICUNA = config.applications['vicuna'],
        BWA = config.applications['bwa'],
        INDELFIXER = config.applications['indelfixer'],
        CONSENSUSFIXER = config.applications['consensusfixer'],
        PICARD = config.applications['picard'],
        SAMTOOLS = config.applications['samtools'],
        WORK_DIR = "{dataset}/initial_consensus",
    log:
        outfile = "{dataset}/initial_consensus/vicuna.out.log",
        errfile = "{dataset}/initial_consensus/vicuna.err.log",
    conda:
        config.initial_vicuna['conda']
    benchmark:
        '{dataset}/initial_consensus/vicuna_consensus.benchmark'
    threads:
        config.initial_vicuna['threads']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"
        source functions.sh

        ERRFILE=$(basename {log.errfile})
        OUTFILE=$(basename {log.outfile})

        # 1. copy initial reference for bwa
        rm -rf {params.WORK_DIR}/
        mkdir -p {params.WORK_DIR}/
        cp {input.global_ref} {params.WORK_DIR}/consensus.fasta
        cd {params.WORK_DIR}

        # 2. create bwa index
        {params.BWA} index consensus.fasta 2> >(tee $ERRFILE >&2)

        # 3. create initial alignment
        {params.BWA} mem -t {threads} consensus.fasta ../preprocessed_data/R{{1,2}}.fastq > first_aln.sam 2> >(tee -a $ERRFILE >&2)
        rm consensus.fasta.*

        # 4. remove unmapped reads
        {params.SAMTOOLS} view -b -F 4 first_aln.sam > mapped.bam 2> >(tee -a $ERRFILE >&2)
        rm first_aln.sam

        # 5. extract reads
        mkdir -p cleaned
        SamToFastq {params.PICARD} I=mapped.bam FASTQ=cleaned/R1.fastq SECOND_END_FASTQ=cleaned/R2.fastq VALIDATION_STRINGENCY=SILENT 2> >(tee -a $ERRFILE >&2)
        rm mapped.bam

        # 6. create config file
        # NOTE: Tabs are required below
		cat > vicuna_config.txt <<- _EOF_
			minMSize	9
			maxOverhangSize	2
			Divergence	8
			max_read_overhang	2
			max_contig_overhang	10
			pFqDir	cleaned/
			batchSize	100000
			LibSizeLowerBound	100
			LibSizeUpperBound	800
			min_output_contig_len	1000
			outputDIR	./
		_EOF_

        # 7. VICUNA
        OMP_NUM_THREADS={threads} {params.VICUNA} vicuna_config.txt > $OUTFILE 2> >(tee -a $ERRFILE >&2)
        rm vicuna_config.txt
        rm -r cleaned/

        # 8. fix broken header
        sed -e 's:>dg-\([[:digit:]]\+\)\s.*:>dg-\1:g' contig.fasta > contig_clean.fasta

        # 9. InDelFixer + ConsensusFixer to polish up consensus
        for i in {{1..3}}
        do
                mv consensus.fasta old_consensus.fasta
                InDelFixer {params.INDELFIXER} -i contig_clean.fasta -g old_consensus.fasta 
>> $OUTFILE 2> >(tee -a $ERRFILE >&2)
                sam2bam {params.SAMTOOLS} reads.sam >> $OUTFILE 2> >(tee $ERRFILE >&2)
                ConsensusFixer {params.CONSENSUSFIXER} -i reads.bam -r old_consensus.fasta -mcc 1 -mic 1 -d -pluralityN 0.01 >> $OUTFILE 2> >(tee $ERRFILE >&2)
        done

        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" consensus.fasta
        echo "" >> consensus.fasta

        # 10. finally, move into place
        mkdir -p ../references
        mv {{,../references/vicuna_}}consensus.fasta
        """

rule initial_vicuna_msa:
    input:
        vicuna_refs
    output:
        "references/initial_aln_gap_removed.fasta"
    params:
        scratch = '1250',
        mem = config.initial_vicuna_msa['mem'],
        time = config.initial_vicuna_msa['time'],
        MAFFT = config.applications['mafft'],
        REMOVE_GAPS = config.applications['remove_gaps_msa'],
    log:
        outfile = "references/MAFFT_initial_aln.out.log",
        errfile = "references/MAFFT_initial_aln.err.log",
    conda:
        config.initial_vicuna_msa['conda']
    benchmark:
        "references/MAFFT_initial_aln.benchmark"
    threads:
        config.initial_vicuna_msa['threads']
    shell:
        """
        cat {input} > initial_ALL.fasta
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} initial_ALL.fasta > references/initial_aln.fasta 2> >(tee {log.errfile} >&2)
        rm initial_ALL.fasta

        {params.REMOVE_GAPS} references/initial_aln.fasta -o {output} -p 0.5 > {log.outfile} 2> >(tee -a {log.errfile} >&2)
        """

localrules:
    create_vicuna_initial
rule create_vicuna_initial:
    input:
        "references/initial_aln_gap_removed.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    params:
        EXTRACT_SEQ = config.applications['extract_seq'],
    conda:
        config.create_vicuna_initial['conda']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        {params.EXTRACT_SEQ} {input} -o {output} -s "${{CONSENSUS_NAME}}"
        """

localrules:
    create_simple_initial
rule create_simple_initial:
    input:
        "references/cohort_consensus.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        cp {input} {output}
        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" {output}
        """

localrules:
    create_denovo_initial
rule create_denovo_initial:
    input:
        "{dataset}/references/denovo_consensus.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        cp {input} {output}
        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" {output}
        """

rule vicunaclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/initial_consensus
        rm -rf {params.DIR}/*/*/references/vicuna_consensus.fasta
        rm -rf {params.DIR}/*/*/references/initial_consensus.fasta
        rm -rf references/initial_aln.fasta
        rm -rf references/initial_aln_gap_removed.fasta
        rm -rf references/MAFFT_initial_aln.*
        """

# change this to switch between VICUNA and creating a simple initial
# initial reference
ruleorder:
    create_denovo_initial > create_simple_initial > create_vicuna_initial
# ruleorder: create_vicuna_initial > create_simple_initial


# 4. aligning
rule hmm_align:
    input:
        initial_ref = "{dataset}/references/initial_consensus.fasta",
        R1 = "{dataset}/preprocessed_data/R1.fastq",
        R2 = "{dataset}/preprocessed_data/R2.fastq",
    output:
        good_aln = temp("{dataset}/alignments/full_aln.sam"),
        reject_aln = temp("{dataset}/alignments/rejects.sam"),
        REF_ambig = "{dataset}/references/ref_ambig.fasta",
        REF_majority = "{dataset}/references/ref_majority.fasta",
    params:
        scratch = '1250',
        mem = config.hmm_align['mem'],
        time = config.hmm_align['time'],
        LEAVE_TEMP = '-l' if config.hmm_align['leave_msa_temp'] else '',
        MAFFT = config.applications['mafft'],
        NGSHMMALIGN = config.applications['ngshmmalign'],
    log:
        outfile = "{dataset}/alignments/ngshmmalign.out.log",
        errfile = "{dataset}/alignments/ngshmmalign.err.log",
    conda:
        config.hmm_align['conda']
    benchmark:
        "{dataset}/alignments/ngshmmalign.benchmark"
    threads:
        config.hmm_align['threads']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        # 1. clean previous run
        rm -rf   {wildcards.dataset}/alignments
        rm -f    {wildcards.dataset}/references/ref_ambig.fasta
        rm -f    {wildcards.dataset}/references/ref_majority.fasta
        mkdir -p {wildcards.dataset}/alignments
        mkdir -p {wildcards.dataset}/references

        # 2. perform alignment # -l = leave temps
        {params.NGSHMMALIGN} -v -R {input.initial_ref} -o {output.good_aln} -w {output.reject_aln} -t {threads} -N "${{CONSENSUS_NAME}}" {params.LEAVE_TEMP} {input.R1} {input.R2} > {log.outfile} 2> >(tee {log.errfile} >&2)

        # 3. move references into place
        mv {wildcards.dataset}/{{alignments,references}}/ref_ambig.fasta
        mv {wildcards.dataset}/{{alignments,references}}/ref_majority.fasta
        """

rule sam2bam:
    input:
        "{file}.sam"
    output:
        BAM = "{file}.bam",
        BAI = "{file}.bam.bai"
    params:
        scratch = '1250',
        mem = config.sam2bam['mem'],
        time = config.sam2bam['time'],
        SAMTOOLS = config.applications['samtools'],
    log:
        outfile = "{file}_sam2bam.out.log",
        errfile = "{file}_sam2bam.err.log",
    conda:
        config.sam2bam['conda']
    benchmark:
        "{file}_sam2bam.benchmark"
    threads:
        1
    shell:
        """
        # convert sam -> bam
        source functions.sh
        sam2bam {params.SAMTOOLS} {input} > {log.outfile} 2> >(tee {log.errfile} >&2)
        """

# 4a. align against 5VM as a QA check
rule bwa_QA:
    input:
        patient_ref = "{dataset}/references/ref_{kind}.fasta",
        virusmix_ref = "references/5-Virus-Mix.fasta",
        R1 = "{dataset}/preprocessed_data/R1.fastq",
        R2 = "{dataset}/preprocessed_data/R2.fastq",
    output:
        SAM = temp("{dataset}/QA_alignments/bwa_QA_{kind}.sam"),
        MSA = "{dataset}/QA_alignments/bwa_refs_msa_{kind}.fasta",
    params:
        scratch = '1250',
        mem = config.bwa_QA['mem'],
        time = config.bwa_QA['time'],
        BWA = config.applications['bwa'],
        MAFFT = config.applications['mafft'],
    log:
        outfile = "{dataset}/QA_alignments/bwa_{kind}.out.log",
        errfile = "{dataset}/QA_alignments/bwa_{kind}.err.log",
    conda:
        config.bwa_QA['conda']
    benchmark:
        "{dataset}/QA_alignments/bwa_{kind}.benchmark"
    threads:
        config.bwa_QA['threads']
    shell:
        """
        # 1. cleanup old run
        rm -f {output.SAM} {output.MSA}

        # 2. concatenate references
        mkdir -p {wildcards.dataset}/QA_alignments
        cat {input.patient_ref} {input.virusmix_ref} > {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta

        # 3. indexing
        {params.BWA} index {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta 2> >(tee {log.errfile} >&2)

        # 4. align
        {params.BWA} mem -t {threads} {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta {input.R1} {input.R2} > {output.SAM} 2> >(tee -a {log.errfile} >&2)

        # 5. MSA
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta > {output.MSA} 2> >(tee -a {log.errfile} >&2)

        # 6. cleanup BWA indices
        rm -f {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta.*
        """

# 4b. Call coverage statistics
rule coverage_QA:
    input:
        BAM = "{dataset}/QA_alignments/bwa_QA_{kind}.bam",
        MSA = "{dataset}/QA_alignments/bwa_refs_msa_{kind}.fasta",
    output:
        "{dataset}/QA_alignments/coverage_{kind}.tsv",
    params:
        scratch = '1250',
        mem = config.coverage_QA['mem'],
        time = config.coverage_QA['time'],
        COV_STATS = config.applications['coverage_stats'],
    log:
        outfile = "{dataset}/QA_alignments/coverage_QA_{kind}.out.log",
        errfile = "{dataset}/QA_alignments/coverage_QA_{kind}.err.log",
    conda:
        config.coverage_QA['conda']
    benchmark:
        "{dataset}/QA_alignments/coverage_QA_{kind}.benchmark"
    threads:
        1
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        # 1. clean previous run
        rm -f {output}
        mkdir -p {wildcards.dataset}/QA_alignments

        # 2. collect coverage stats
        # we only collect statistics in the loop regions
        # of HIV-1 in order
        {params.COV_STATS} -t HXB2:6614-6812,7109-7217,7376-7478,7601-7634 -i {input.BAM} -o {output} -m {input.MSA} --select "${{CONSENSUS_NAME}}" > {log.outfile} 2> >(tee {log.errfile} >&2)
        """


# 5. construct MSA from all patient files
def construct_msa_input_files(wildcards):
    output_list = ["{}{}.fasta".format(s, wildcards.kind)
                   for s in references]
    output_list.append(reference_file)

    return output_list


rule msa:
    input:
        construct_msa_input_files
    output:
        "references/ALL_aln_{kind}.fasta"
    params:
        scratch = '1250',
        mem = config.msa['mem'],
        time = config.msa['time'],
        MAFFT = config.applications['mafft'],
    log:
        outfile = "references/MAFFT_{kind}_cohort.out.log",
        errfile = "references/MAFFT_{kind}_cohort.err.log",
    conda:
        config.msa['conda']
    benchmark:
        "references/MAFFT_{kind}_cohort.benchmark"
    threads:
        config.msa['threads']
    shell:
        """
        cat {input} > ALL_{wildcards.kind}.fasta
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} ALL_{wildcards.kind}.fasta > {output} 2> >(tee {log.errfile} >&2)
        rm ALL_{wildcards.kind}.fasta
        """

rule msaclean:
    shell:
        """
        rm -rf references/ALL_aln_*.fasta
        rm -rf references/MAFFT_*_cohort.*
        """


# 6. convert alignments to REF alignment
rule convert_to_ref:
    input:
        REF_ambig = "references/ALL_aln_ambig.fasta",
        REF_majority = "references/ALL_aln_majority.fasta",
        BAM = "{dataset}/alignments/full_aln.bam",
        REJECTS_BAM = "{dataset}/alignments/rejects.bam",
    output:
        "{dataset}/alignments/REF_aln.bam"
    params:
        scratch = '1250',
        mem = config.convert_to_ref['mem'],
        time = config.convert_to_ref['time'],
        REF_NAME = reference_name,
        CONVERT_REFERENCE = config.applications['convert_reference'],
    log:
        outfile = "{dataset}/alignments/convert_to_ref.out.log",
        errfile = "{dataset}/alignments/convert_to_ref.err.log",
    conda:
        config.convert_to_ref['conda']
    benchmark:
        "{dataset}/alignments/convert_to_ref.benchmark"
    threads:
        1
    shadow:
        "shallow"
    shell:
        """
        {params.CONVERT_REFERENCE} -t {params.REF_NAME} -m {input.REF_ambig} -i {input.BAM} -o {output} > {log.outfile} 2> >(tee {log.errfile} >&2)
        """


rule alignclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/alignments
        rm -rf {params.DIR}/*/*/QA_alignments
        rm -rf {params.DIR}/*/*/references/ref_ambig.fasta
        rm -rf {params.DIR}/*/*/references/ref_majority.fasta
        rm -rf {params.DIR}/*/*/references/initial_consensus.fasta
        """

# 4-6. Alternative: align reads using bwa
rule ref_index:
    input:
        reference_file
    output:
        "{}.bwt".format(reference_file)
    params:
        scratch = '1250',
        mem = config.ref_index['mem'],
        time = config.ref_index['time'],
        BWA = config.applications['bwa']
    log:
        outfile = "references/bwa_index.out.log",
        errfile = "references/bwa_index.err.log",
    conda:
        config.ref_index['conda']
    benchmark:
        'references/ref_index.benchmark'
    shell:
        """
        {params.BWA} index {input} 2> >(tee {log.errfile} >&2)
        """

rule bwa_align:
    input:
        R1 = "{dataset}/preprocessed_data/R1.fastq",
        R2 = "{dataset}/preprocessed_data/R2.fastq",
        REF = reference_file,
        INDEX = "{}.bwt".format(reference_file)
    output:
        temp("{dataset}/alignments/REF_aln.sam")
    params:
        scratch = '1250',
        mem = config.bwa_align['mem'],
        time = config.bwa_align['time'],
        REF_NAME = reference_name,
        TMP_SAM = "{dataset}/alignments/tmp_aln.sam",
        BWA = config.applications['bwa'],
        SAMTOOLS = config.applications['samtools'],
    log:
        outfile = "{dataset}/alignments/bwa_align.out.log",
        errfile = "{dataset}/alignments/bwa_align.err.log",
    conda:
        config.bwa_align['conda']
    benchmark:
        "{dataset}/alignments/bwa_align.benchmark"
    threads:
        config.bwa_align['threads']
    shell:
        """
        {params.BWA} mem -t {threads} {input.REF} {input.R1} {input.R2} > {params.TMP_SAM} 2> >(tee {log.errfile} >&2)
        {params.SAMTOOLS} view -h -f 2 -F 2048 {params.TMP_SAM} > {output} 2> >(tee -a {log.errfile} >&2)
        rm {params.TMP_SAM}
        """

rule bwaclean:
    input:
        "{}.bwt".format(reference_file)
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -f {input}
        rm -rf {params.DIR}/*/*/alignments
        """

if config.general["aligner"] == "ngshmmalign":
    ruleorder: convert_to_ref > sam2bam
elif config.general["aligner"] == "bwa":
    ruleorder: sam2bam > convert_to_ref

# 7. Ouptut minor allele frequencies
rule minor_variants:
    input:
        REF = reference_file,
        BAM = expand("{dataset}/alignments/REF_aln.bam", dataset=datasets),
    output:
        VARIANTS = "variants/minority_variants.tsv",
        CONSENSUS = "variants/cohort_consensus.fasta",
        COVERAGE = "variants/coverage.tsv"
    params:
        scratch = '1250',
        mem = config.minor_variants['mem'],
        time = config.minor_variants['time'],
        OUTDIR = "variants",
        NAMES = IDs,
        MINORITY_CALLER = config.applications['minority_freq'],
    log:
        outfile = "variants/minority_variants.out.log",
        errfile = "variants/minority_variants.out.log",
    conda:
        config.minor_variants['conda']
    benchmark:
        'variants/minority_variants.benchmark'
    threads:
        config.minor_variants['threads']
    shell:
        """
        {params.MINORITY_CALLER} -r {input.REF} -N {params.NAMES} -t {threads} -o {params.OUTDIR} -d {input.BAM} > >(tee {log.outfile}) 2>&1 
        """

# 8. Call single nucleotide variants
def window_lengths(wildcards):
    window_len = []
    for p in patient_list:
        read_len = patient_dict[p]
        aux = int((read_len * 4/5 + config.snv['shift']) /  config.snv['shift'])
        window_len.append(str(aux * config.snv['shift']))

    window_len = ','.join(window_len)
    return window_len


rule coverage_intervals:
    input:
        REF = reference_file,
        TSV = "variants/coverage.tsv"
    output:
        "variants/coverage_intervals.tsv"
    params:
        scratch = '1250',
        mem = config.coverage_intervals['mem'],
        time = config.coverage_intervals['time'],
        WINDOW_LEN = window_lengths,
        COVERAGE = config.coverage_intervals['coverage'],
        SHIFT = config.snv['shift'],
        NAMES = IDs,
        LIBERAL = '-e' if config.coverage_intervals['liberal'] else '',
        EXTRACT_COVERAGE_INTERVALS = config.applications['extract_coverage_intervals']
    log:
        outfile = "variants/coverage_intervals.out.log",
        errfile = "variants/coverage_intervals.out.log",
    conda:
        config.coverage_intervals['conda']
    benchmark:
        'variants/coverage_intervals.benchmark'
    threads:
        1
    shell:
        """
        {params.EXTRACT_COVERAGE_INTERVALS} -i {input.TSV} -r {input.REF} -c {params.COVERAGE} -w {params.WINDOW_LEN} -N {params.NAMES} {params.LIBERAL} -o {output} > >(tee {log.outfile}) 2>&1  
        """

localrules:
    shorah_regions
rule shorah_regions:
    input:
        "variants/coverage_intervals.tsv"
    output:
        temp(
            expand("{dataset}/variants/coverage_intervals.tsv", dataset=datasets))
    params:
        scratch = '1250',
    threads:
        1
    run:
        with open(input[0], 'r') as infile:
            for line in infile:
                parts = line.rstrip().split('\t')
                patientID = parts[0].split('-')
                sample_date = patientID[-1]
                patientID = '-'.join(patientID[:-1])
                if len(parts) == 2:
                    regions = parts[1].split(',')
                else:
                    regions = []

                with open(os.path.join(config.input['datadir'], patientID, sample_date, "variants", "coverage_intervals.tsv"), 'w') as outfile:
                    outfile.write('\n'.join(regions))


def read_len(wildcards):
    parts = wildcards.dataset.split('/')
    patient_ID = parts[1]
    date = parts[2]
    patient_tuple = patient_record(patient_id=patient_ID, date=date)
    read_len = patient_dict[patient_tuple]
    return read_len


rule snv:
    input:
        REF = "variants/cohort_consensus.fasta",
        BAM = "{dataset}/alignments/REF_aln.bam",
        TSV = "{dataset}/variants/coverage_intervals.tsv",
    output:
        "{dataset}/variants/SNVs/snvs.csv"
    params:
        scratch = '1250',
        mem = config.snv['mem'],
        time = config.snv['time'],
        READ_LEN = read_len,
        SHIFT = config.snv['shift'],
        KEEP_FILES = 'true' if config.snv['keep_files'] else 'false',
        WORK_DIR = "{dataset}/variants/SNVs",
        SHORAH = config.applications['shorah']
    log:
        outfile = "{dataset}/variants/SNVs/shorah.out.log",
        errfile = "{dataset}/variants/SNVs/shorah.err.log",
    conda:
        config.snv['conda']
    benchmark:
        "{dataset}/variants/SNVs/shorah.benchmark"
    threads:
        config.snv['threads']
    shell:
        """
        let "WINDOW_SHIFTS=({params.READ_LEN} * 4/5 + {params.SHIFT}) / {params.SHIFT}"
        let "WINDOW_LEN=WINDOW_SHIFTS * {params.SHIFT}"

        echo "Windows are shifted by: ${{WINDOW_SHIFTS}} bp" > {log.outfile}
        echo "The window length is: ${{WINDOW_LEN}} bp" >> {log.outfile}

        # Get absolute path for input files
        CWD=${{PWD}}
        BAM=${{PWD}}/{input.BAM}
        REF=${{PWD}}/{input.REF}
        OUTFILE=${{PWD}}/{log.outfile}
        ERRFILE=${{PWD}}/{log.errfile}
        WORK_DIR=${{PWD}}/{params.WORK_DIR}

        # Run ShoRAH in each of the predetermined regions (regions with sufficient coverage)
        LINE_COUNTER=0
        FILES=""
        while read -r region || [[ -n ${{region}} ]]
        do
            echo "Running ShoRAH on region: ${{region}}" >> $OUTFILE
            LINE_COUNTER=$(( $LINE_COUNTER + 1))
            # Create directory for running ShoRAH in a corresponding region (if doesn't exist)
            DIR=${{WORK_DIR}}/REGION_${{LINE_COUNTER}}
            if [[ ! -d "${{DIR}}" ]]; then
                echo "Creating directory ${{DIR}}" >> $OUTFILE
                mkdir -p ${{DIR}}
            else
                # Results from previous runs
                if {params.KEEP_FILES}; then
                    DIR_DST=${{WORK_DIR}}/old
                    echo "Moving results from a previous run to ${{DIR_DST}}" >> $OUTFILE
                    rm -rf ${{DIR_DST}}/REGION_${{LINE_COUNTER}}
                    mkdir -p ${{DIR_DST}}
                    mv -f ${{DIR}} ${{DIR_DST}}
                    mkdir -p ${{DIR}}
                fi
            fi
            # Change to the directory where ShoRAH is to be executed
            cd ${{DIR}}

            {params.SHORAH} -w ${{WINDOW_LEN}} -x 100000 -r ${{region}} -S 42 -b ${{BAM}} -f ${{REF}} >> $OUTFILE 2> >(tee -a $ERRFILE >&2)
            if [[ -f ${{DIR}}/snv/SNVs_0.010000_final.csv ]]; then
                FILES="$FILES ${{DIR}}/snv/SNVs_0.010000_final.csv"
            else
                echo "ERROR: unsuccesful execution of ShoRAH" 2> >(tee -a $ERRFILE >&2)
                exit 1
            fi

            # Change back to working directory
            cd ${{CWD}}
        done < {input.TSV}

        # Aggregate results from different regions
        if [[ -z ${{FILES}} ]]; then
            if [[ ${{LINE_COUNTER}} > 0 ]]; then
                echo "ERROR: unsuccesful execution of ShoRAH" 2> >(tee -a {log.errfile} >&2)
                exit 1
            else
                echo "No alignment region reports sufficient coverage" >> {log.outfile}
                touch {output}
            fi
        else
            echo "Intermediate files: ${{FILES}}" >> {log.outfile}
            cat ${{FILES}} | sort -t, -nk2 | tail -n +${{LINE_COUNTER}} > {output}
        fi
        """

rule snvclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/variants/SNVs
        """

rule haploclique:
    input:
        "{dataset}/alignments/REF_aln.bam"
    output:
        FASTA = "{dataset}/variants/global/quasispecies.fasta",
        BAM = "{dataset}/variants/global/quasispecies.bam",
    params:
        scratch = '1250',
        mem = config.haploclique['mem'],
        time = config.haploclique['time'],
        RELAX = '--edge_quasi_cutoff_cliques=0.85 --edge_quasi_cutoff_mixed=0.85 --edge_quasi_cutoff_single=0.8 --min_overlap_cliques=0.6 --min_overlap_single=0.5' if config.haploclique[
            'relax'] else '',
        NO_SINGLETONS = '--no_singletons' if config.haploclique['no_singletons'] else '',
        NO_PROB0 = '--no_prob0' if config.haploclique['no_prob0'] else '',
        CLIQUE_SIZE_LIMIT = config.haploclique['clique_size_limit'],
        MAX_NUM_CLIQUES = config.haploclique['max_num_cliques'],
        OUTPREFIX = "{dataset}/variants/global/quasispecies",
        HAPLOCLIQUE = config.applications['haploclique'],
    log:
        outfile = "{dataset}/variants/global/haploclique.out.log",
        errfile = "{dataset}/variants/global/haploclique.err.log",
    conda:
        config.haploclique['conda']
    benchmark:
        "{dataset}/variants/global/haploclique.benchmark"
    threads:
        1
    shell:
        """
        {params.HAPLOCLIQUE} {params.RELAX} {params.NO_SINGLETONS} {params.NO_PROB0} --limit_clique_size={params.CLIQUE_SIZE_LIMIT} --max_cliques={params.MAX_NUM_CLIQUES} --log={log.outfile} --bam {input} {params.OUTPREFIX} 2> >(tee {log.errfile} >&2)
        """

rule haploclique_visualization:
    input:
        BAM = "{dataset}/variants/global/quasispecies.bam",
        FASTA = "{dataset}/variants/global/quasispecies.fasta",
    output:
        PDF = "{dataset}/variants/global/quasispecies_plot.pdf",
    params:
        scratch = '1250',
        mem = config.haploclique_visualization['mem'],
        time = config.haploclique_visualization['time'],
        REGION_START = config.haploclique_visualization['region_start'],
        REGION_END = config.haploclique_visualization['region_end'],
        USE_MSA = '-r' if len(
            config.haploclique_visualization['msa']) > 0 else '',
        MSA = config.haploclique_visualization['msa'],
        TSV = "{dataset}/variants/global/quasispecies_mapping.tsv",
        INPREFIX = "{dataset}/variants/global/quasispecies",
        COMPUTE_MDS = config.applications['compute_mds'],
    log:
        outfile = "{dataset}/variants/global/haploclique_visualization.out.log",
        errfile = "{dataset}/variants/global/haploclique_visualization.err.log",
    conda:
        config.haploclique_visualization['conda']
    benchmark:
        "{dataset}/variants/global/haploclique_visualization.benchmark"
    threads:
        1
    shell:
        """
        {params.COMPUTE_MDS} -q {params.INPREFIX} -s {params.REGION_START} -e {params.REGION_END} {params.USE_MSA} {params.MSA} -p {output.PDF} -o {params.TSV} > {log.output} 2> >(tee {log.errfile} >&2)
        """

rule haplocliqueclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm {params.DIR}/*/*/variants/global/quasispecies.*
        """
